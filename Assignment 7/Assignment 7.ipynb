{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b690f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e269ac6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TfidVectorizer' from 'sklearn.feature_extraction.text' (C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_416/878477794.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TfidVectorizer' from 'sklearn.feature_extraction.text' (C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb45907",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc1.txt','r') as fp1:\n",
    "    s1=fp1.read()\n",
    "with open('doc2.txt','r') as fp2:\n",
    "    s2=fp2.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb2f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words after tokenization are\n",
      "['I', 'meet', 'Jack', 'and', 'I', 'meet', 'Ann']\n",
      "['I', 'meet', 'Jack', 'and', 'I', 'meet', 'Jill', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "l1=word_tokenize(s1)\n",
    "l2=word_tokenize(s2)\n",
    "print(\"List of words after tokenization are\")\n",
    "print(l1)\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15379703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words after stop words removal is\n",
      "['I', 'meet', 'Jack', 'I', 'meet', 'Ann']\n",
      "['I', 'meet', 'Jack', 'I', 'meet', 'Jill', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stop words removal\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "def rem_stopwords(l):\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    filteredl=[]\n",
    "    for w in l:\n",
    "        if w not in stop_words:\n",
    "            filteredl.append(w)\n",
    "    return filteredl\n",
    "filtered1=rem_stopwords(l1)\n",
    "filtered2=rem_stopwords(l2)\n",
    "print(\"List of words after stop words removal is\")\n",
    "print(filtered1)\n",
    "print(filtered2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0e17c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words after lemmatizing\n",
      "['I', 'meet', 'Jack', 'I', 'meet', 'Ann']\n",
      "['I', 'meet', 'Jack', 'I', 'meet', 'Jill', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization.\n",
    "def lem(l):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemmatizedl=[]\n",
    "    for w in l:\n",
    "        wl=lemmatizer.lemmatize(w)\n",
    "        lemmatizedl.append(wl)\n",
    "    return lemmatizedl\n",
    "lemmatizedl1=lem(filtered1)\n",
    "lemmatizedl2=lem(filtered2)\n",
    "\n",
    "print(\"List of words after lemmatizing\")\n",
    "print(lemmatizedl1)\n",
    "print(lemmatizedl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bb39cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words with POS tagging is\n",
      "[('I', 'PRP'), ('meet', 'VBP'), ('Jack', 'NNP'), ('I', 'PRP'), ('meet', 'VBP'), ('Ann', 'JJ')]\n",
      "[('I', 'PRP'), ('meet', 'VBP'), ('Jack', 'NNP'), ('I', 'PRP'), ('meet', 'VBP'), ('Jill', 'NNP'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#POS Tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged1=nltk.pos_tag(lemmatizedl1)\n",
    "tagged2=nltk.pos_tag(lemmatizedl2)\n",
    "print(\"List of words with POS tagging is\")\n",
    "print(tagged1)\n",
    "print(tagged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aa4798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words with frequency count\n",
      "[('I', 2), ('meet', 2), ('Jack', 1), ('Ann', 1)]\n",
      "[('I', 2), ('meet', 2), ('Jack', 1), ('Jill', 1), ('.', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Term Frequency\n",
    "fdist1=FreqDist(lemmatizedl1)\n",
    "count1=len(fdist1)\n",
    "fdist2=FreqDist(lemmatizedl2)\n",
    "count2=len(fdist2)\n",
    "print(\"List of words with frequency count\")\n",
    "print(fdist1.most_common(count1))\n",
    "print(fdist2.most_common(count2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c9a8569",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_416/4254892134.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtfidfvectorizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtfidf_wm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidfvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtfidf_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidfvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf_tfidvect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidf_wm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Doc1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Doc2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidf_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TF-IDF Vectorizer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "# Term frequency TF-IDF\n",
    "corpus=[s1,s2]\n",
    "tfidfvectorizer=TfidfVectorizer(analyzer=\"word\",stop_words=\"english\")\n",
    "tfidf_wm=tfidfvectorizer.fit_transform(corpus)\n",
    "tfidf_tokens=tfidfvectorizer.get_feature_names_out()\n",
    "df_tfidvect=pd.DataFrame(data=tfidf_wm.toarray(),index=[\"Doc1\",\"Doc2\"],columns=tfidf_tokens)\n",
    "print(\"TF-IDF Vectorizer\")\n",
    "print(df_tfidvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9d3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8e6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
